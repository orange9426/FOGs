import sys
sys.path.append(sys.path[0] + '/..')

import collections

from policy.policy import TabularPolicy
# def _memoize_method(method):
#     cache_name = "cache_" + method.__name__

#     def wrap(self, arg):
#         key = str(arg)
#         cache = vars(self).setdefault(cache_name, {})
#         if key not in cache:
#             cache[key] = method(self, arg)
#         return cache[key]

#     return wrap

class BRPolicy(TabularPolicy):
    def __init__(self, game, player_id, policy, root_history=None):
        self._num_players = game.num_players
        self._player_id = player_id
        self._policy = policy
        if root_history is None:
            root_history = game.initial_history()
        self._root_history = root_history
        self.infosets = self.info_sets(root_history)

    def info_sets(self, history):
        infosets = collections.defaultdict(list)
        for s, p in self.decision_nodes(history):
            infosets[s.get_info_state()[self._player_id].to_string()].append((s, p))
        return dict(infosets)

    def decision_nodes(self, parent_history):
        if not parent_history.is_terminal():
            if parent_history.current_player() == self._player_id:
                yield (parent_history, 1.0)
            for action, p_action in self.transitions(parent_history):
                for state, p_state in self.decision_nodes(parent_history.child(action)):
                    yield (state, p_state * p_action)

    def transitions(self, history):
        if history.current_player() == self._player_id:
            return [(action, 1.0) for action in history.legal_actions()]
        elif history.is_chance():
            return zip(*history.chance_outcomes())
        else:
            return zip(*self._policy.action_probabilities(history, history.current_player()))
        
    def value(self, history):
        if history.is_terminal():
            return history.get_return()*(1-2*self._player_id)
        elif history.current_player() == self._player_id:
            action = self.br_action(history.get_info_state()[self._player_id].to_string())
            return self.q_value(history, action)
        else:
            return sum(p * self.q_value(history, a) for a, p in self.transitions(history))
        
    def q_value(self, history, action):
        return self.value(history.child(action))

    def br_action(self, infostate):
        infoset = self.infosets[infostate]
        return max(infoset[0][0].legal_actions(), key=lambda a: sum(cf_p * self.q_value(s, a) for s, cf_p in infoset))
    
    #TODO action_probabilities

def exploitability(game, policy):
    root_history = game.initial_history()
    nash_conv_value = sum(BRPolicy(game, best_responder, policy, root_history).value(root_history) for best_responder in range(game.num_players))
    return nash_conv_value / game.num_players
